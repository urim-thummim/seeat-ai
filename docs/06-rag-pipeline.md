# RAG íŒŒì´í”„ë¼ì¸ ìƒì„¸ ì„¤ê³„

## ğŸ”„ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ê°œìš”

```mermaid
graph TD
    A[ìœ íŠœë¸Œ ì˜ìƒ] --> B[yt-dlp ë‹¤ìš´ë¡œë“œ]
    B --> C[ffmpeg ìŒì„± ì¶”ì¶œ]
    C --> D[Whisper STT]
    D --> E[í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬]
    E --> F[ì²­í‚¹ & ë©”íƒ€ë°ì´í„°]
    F --> G[ì„ë² ë”© ìƒì„±]
    G --> H[ë²¡í„° DB ì €ì¥]
    
    I[ì‚¬ìš©ì ì§ˆë¬¸] --> J[ì§ˆë¬¸ ì„ë² ë”©]
    J --> K[ë²¡í„° ê²€ìƒ‰]
    H --> K
    K --> L[ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ]
    L --> M[LLM í”„ë¡¬í”„íŠ¸]
    M --> N[ë‹µë³€ ìƒì„±]
    N --> O[ì¶œì²˜ ì •ë³´ ì¶”ê°€]
    O --> P[ìµœì¢… ë‹µë³€]
```

## ğŸ“¥ 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬

### 1.1 ìœ íŠœë¸Œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ
```python
# yt-dlp ì„¤ì •
ydl_opts = {
    'format': 'bestaudio/best',
    'outtmpl': 'data/raw_videos/%(title)s.%(ext)s',
    'extractaudio': True,
    'audioformat': 'mp3',
    'audioquality': '192',
    'writeinfojson': True,  # ë©”íƒ€ë°ì´í„° ì €ì¥
    'writesubtitles': False,  # ìë§‰ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (STT ì‚¬ìš©)
}

# ë‹¤ìš´ë¡œë“œ í”„ë¡œì„¸ìŠ¤
def download_sermon_video(youtube_url: str) -> SermonMetadata:
    """
    ìœ íŠœë¸Œ ì˜ìƒì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œ
    """
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(youtube_url, download=True)
        
    return SermonMetadata(
        youtube_id=info['id'],
        title=info['title'],
        duration=info['duration'],
        upload_date=info['upload_date'],
        description=info['description'],
        thumbnail_url=info['thumbnail']
    )
```

### 1.2 ìŒì„± í’ˆì§ˆ ìµœì í™”
```python
# ffmpeg ë¥¼ ì‚¬ìš©í•œ ìŒì„± ì „ì²˜ë¦¬
def preprocess_audio(input_file: str, output_file: str):
    """
    ìŒì„± í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì „ì²˜ë¦¬
    """
    cmd = [
        'ffmpeg', '-i', input_file,
        '-af', 'highpass=f=100,lowpass=f=8000',  # ì£¼íŒŒìˆ˜ í•„í„°ë§
        '-af', 'volume=1.5',  # ë³¼ë¥¨ ì¦í­
        '-ar', '16000',  # ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Whisper ìµœì í™”)
        '-ac', '1',  # ëª¨ë…¸ ì±„ë„
        '-y', output_file
    ]
    subprocess.run(cmd, check=True)
```

### 1.3 STT ì²˜ë¦¬ (Whisper)
```python
# Whisper ìµœì í™” ì„¤ì •
whisper_options = {
    "model": "large-v3",
    "language": "ko",
    "task": "transcribe",
    "temperature": 0.0,  # ì¼ê´€ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
    "beam_size": 5,
    "best_of": 5,
    "condition_on_previous_text": True,
    "compression_ratio_threshold": 2.4,
    "logprob_threshold": -1.0,
    "no_speech_threshold": 0.6,
    "word_timestamps": True,  # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
}

def transcribe_audio(audio_file: str) -> TranscriptResult:
    """
    Whisperë¥¼ ì‚¬ìš©í•œ STT ì²˜ë¦¬
    """
    model = whisper.load_model("large-v3")
    result = model.transcribe(
        audio_file,
        **whisper_options
    )
    
    # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì²˜ë¦¬
    segments = []
    for segment in result["segments"]:
        segments.append(TranscriptSegment(
            start_time=segment["start"],
            end_time=segment["end"],
            text=segment["text"].strip(),
            words=segment.get("words", [])
        ))
    
    return TranscriptResult(
        full_text=result["text"],
        segments=segments,
        language=result["language"]
    )
```

## ğŸ“ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì²­í‚¹

### 2.1 í…ìŠ¤íŠ¸ ì •ì œ ë° ì •ê·œí™”
```python
import re
from kiwipiepy import Kiwi

def clean_and_normalize_text(text: str) -> str:
    """
    ì„¤êµ í…ìŠ¤íŠ¸ ì •ì œ ë° ì •ê·œí™”
    """
    # ë°˜ë³µë˜ëŠ” ìŒì„± ì¸ì‹ ì˜¤ë¥˜ íŒ¨í„´ ì œê±°
    text = re.sub(r'\b(\w+)\s+\1\b', r'\1', text)  # ì¤‘ë³µ ë‹¨ì–´ ì œê±°
    
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    text = re.sub(r'[^\w\sê°€-í£.,!?]', '', text)
    
    # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ë¬¸ì¥ ê²½ê³„ ê°œì„ 
    kiwi = Kiwi()
    sentences = kiwi.split_into_sents(text)
    
    return ' '.join([sent.text.strip() for sent in sentences])
```

### 2.2 ì§€ëŠ¥í˜• ì²­í‚¹ ì „ëµ
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

class SermonAwareTextSplitter:
    """
    ì„¤êµ íŠ¹ì„±ì„ ê³ ë ¤í•œ í…ìŠ¤íŠ¸ ë¶„í• ê¸°
    """
    
    def __init__(self):
        self.base_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # í† í° ìˆ˜ ê³ ë ¤ (ì•½ 600 í† í°)
            chunk_overlap=100,  # ë¬¸ë§¥ ì—°ê²°ì„ ìœ„í•œ ì˜¤ë²„ë©
            separators=[
                "\n\n",  # ë¬¸ë‹¨ êµ¬ë¶„
                "ì•„ë©˜.",   # ê¸°ë„ êµ¬ë¶„
                "ì„±ê²½ë§ì”€",  # ì„±ê²½ êµ¬ì ˆ êµ¬ë¶„
                ".",      # ë¬¸ì¥ êµ¬ë¶„
                "!",
                "?",
                "\n",
                " "
            ]
        )
    
    def split_sermon_text(
        self, 
        text: str, 
        segments: List[TranscriptSegment]
    ) -> List[SermonChunk]:
        """
        ì„¤êµ ë‚´ìš©ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
        """
        chunks = []
        current_position = 0
        
        # ê¸°ë³¸ ì²­í‚¹
        base_chunks = self.base_splitter.split_text(text)
        
        for i, chunk_text in enumerate(base_chunks):
            # í•´ë‹¹ ì²­í¬ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì°¾ê¸°
            start_time, end_time = self._find_chunk_timestamps(
                chunk_text, text, segments, current_position
            )
            
            # ì£¼ì œ ë¶„ë¥˜ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
            topic = self._classify_topic(chunk_text)
            
            # ì„±ê²½ êµ¬ì ˆ ì¶”ì¶œ
            bible_verses = self._extract_bible_verses(chunk_text)
            
            chunk = SermonChunk(
                chunk_id=f"chunk_{i:03d}",
                text=chunk_text,
                start_time=start_time,
                end_time=end_time,
                topic=topic,
                bible_verses=bible_verses,
                chunk_index=i
            )
            
            chunks.append(chunk)
            current_position += len(chunk_text)
        
        return chunks
    
    def _find_chunk_timestamps(
        self, 
        chunk_text: str, 
        full_text: str, 
        segments: List[TranscriptSegment],
        start_pos: int
    ) -> Tuple[float, float]:
        """
        ì²­í¬ì˜ ì‹œì‘/ë íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
        """
        chunk_start_pos = full_text.find(chunk_text, start_pos)
        chunk_end_pos = chunk_start_pos + len(chunk_text)
        
        start_time = None
        end_time = None
        
        for segment in segments:
            segment_start_pos = full_text.find(segment.text)
            segment_end_pos = segment_start_pos + len(segment.text)
            
            # ì²­í¬ ì‹œì‘ì ì´ ì´ ì„¸ê·¸ë¨¼íŠ¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°
            if (chunk_start_pos >= segment_start_pos and 
                chunk_start_pos <= segment_end_pos):
                if start_time is None:
                    start_time = segment.start_time
            
            # ì²­í¬ ëì ì´ ì´ ì„¸ê·¸ë¨¼íŠ¸ì— í¬í•¨ë˜ëŠ” ê²½ìš°
            if (chunk_end_pos >= segment_start_pos and 
                chunk_end_pos <= segment_end_pos):
                end_time = segment.end_time
        
        return start_time or 0.0, end_time or 0.0
    
    def _classify_topic(self, text: str) -> str:
        """
        ì²­í¬ì˜ ì£¼ì œ ë¶„ë¥˜
        """
        topics = {
            "ê¸°ë„": ["ê¸°ë„", "ì£¼ê¸°ë„ë¬¸", "ê°„êµ¬", "ì•„ë©˜"],
            "ì°¬ì–‘": ["ì°¬ì–‘", "ê²½ë°°", "ë…¸ë˜", "í• ë ë£¨ì•¼"],
            "ì„±ê²½": ["ì„±ê²½", "ë§ì”€", "êµ¬ì ˆ", "ì¥", "ì ˆ"],
            "ê°„ì¦": ["ê°„ì¦", "ê²½í—˜", "ì€í˜œ", "ê°ì‚¬"],
            "êµí›ˆ": ["êµí›ˆ", "ê°€ë¥´ì¹¨", "ë°°ì›€", "ê¹¨ë‹¬ìŒ"],
            "ì‚¬ë‘": ["ì‚¬ë‘", "ìš©ì„œ", "ìë¹„", "ê¸íœ¼"],
            "ë¯¿ìŒ": ["ë¯¿ìŒ", "ì‹ ì•™", "í™•ì‹ ", "ì†Œë§"],
            "íšŒê°œ": ["íšŒê°œ", "ì£„", "ìš©ì„œ", "ëŒì´í‚´"]
        }
        
        for topic, keywords in topics.items():
            if any(keyword in text for keyword in keywords):
                return topic
        
        return "ì¼ë°˜"
    
    def _extract_bible_verses(self, text: str) -> List[str]:
        """
        ì„±ê²½ êµ¬ì ˆ ì¶”ì¶œ
        """
        # ì„±ê²½ êµ¬ì ˆ íŒ¨í„´ ë§¤ì¹­
        bible_pattern = r'([ê°€-í£]+ì„œ?)\s*(\d+)ì¥?\s*(\d+)ì ˆ?'
        matches = re.findall(bible_pattern, text)
        
        verses = []
        for book, chapter, verse in matches:
            verses.append(f"{book} {chapter}:{verse}")
        
        return verses
```

## ğŸ§  3ë‹¨ê³„: ì„ë² ë”© ìƒì„± ë° ì €ì¥

### 3.1 ì„ë² ë”© ëª¨ë¸ ì„¤ì •
```python
from sentence_transformers import SentenceTransformer
import numpy as np

class SermonEmbeddingGenerator:
    """
    ì„¤êµ í…ìŠ¤íŠ¸ìš© ì„ë² ë”© ìƒì„±ê¸°
    """
    
    def __init__(self, model_name: str = "paraphrase-multilingual-mpnet-base-v2"):
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
    
    def generate_embeddings(self, chunks: List[SermonChunk]) -> List[np.ndarray]:
        """
        ì²­í¬ë³„ ì„ë² ë”© ìƒì„±
        """
        # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•œ ì„ë² ë”©
        enhanced_texts = []
        
        for chunk in chunks:
            # ë©”íƒ€ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ì— í¬í•¨
            enhanced_text = self._enhance_text_with_metadata(chunk)
            enhanced_texts.append(enhanced_text)
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(
            enhanced_texts,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        return embeddings
    
    def _enhance_text_with_metadata(self, chunk: SermonChunk) -> str:
        """
        ë©”íƒ€ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ì— í¬í•¨í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ
        """
        enhanced_parts = [chunk.text]
        
        # ì£¼ì œ ì •ë³´ ì¶”ê°€
        if chunk.topic != "ì¼ë°˜":
            enhanced_parts.append(f"ì£¼ì œ: {chunk.topic}")
        
        # ì„±ê²½ êµ¬ì ˆ ì •ë³´ ì¶”ê°€
        if chunk.bible_verses:
            bible_info = ", ".join(chunk.bible_verses)
            enhanced_parts.append(f"ì„±ê²½êµ¬ì ˆ: {bible_info}")
        
        return " ".join(enhanced_parts)
```

### 3.2 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
```python
import chromadb
from chromadb.config import Settings

class SermonVectorStore:
    """
    ì„¤êµ ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬
    """
    
    def __init__(self, persist_directory: str = "data/chromadb"):
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
    def create_sermon_collection(self, sermon_id: str) -> chromadb.Collection:
        """
        ì„¤êµë³„ ì»¬ë ‰ì…˜ ìƒì„±
        """
        collection_name = f"sermon_{sermon_id}"
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
        try:
            self.client.delete_collection(collection_name)
        except:
            pass
        
        collection = self.client.create_collection(
            name=collection_name,
            metadata={"sermon_id": sermon_id}
        )
        
        return collection
    
    def add_chunks_to_collection(
        self,
        collection: chromadb.Collection,
        chunks: List[SermonChunk],
        embeddings: List[np.ndarray],
        sermon_metadata: SermonMetadata
    ):
        """
        ì²­í¬ì™€ ì„ë² ë”©ì„ ì»¬ë ‰ì…˜ì— ì¶”ê°€
        """
        ids = []
        metadatas = []
        documents = []
        embeddings_list = []
        
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            # ê³ ìœ  ID ìƒì„±
            chunk_id = f"{sermon_metadata.youtube_id}_{chunk.chunk_id}"
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "sermon_id": sermon_metadata.youtube_id,
                "sermon_title": sermon_metadata.title,
                "sermon_date": sermon_metadata.upload_date,
                "chunk_index": chunk.chunk_index,
                "start_time": chunk.start_time,
                "end_time": chunk.end_time,
                "topic": chunk.topic,
                "bible_verses": json.dumps(chunk.bible_verses),
                "youtube_url": f"https://youtube.com/watch?v={sermon_metadata.youtube_id}",
                "timestamp_url": f"https://youtube.com/watch?v={sermon_metadata.youtube_id}&t={int(chunk.start_time)}s"
            }
            
            ids.append(chunk_id)
            metadatas.append(metadata)
            documents.append(chunk.text)
            embeddings_list.append(embedding.tolist())
        
        # ë°°ì¹˜ ì¶”ê°€
        collection.add(
            ids=ids,
            metadatas=metadatas,
            documents=documents,
            embeddings=embeddings_list
        )
```

## ğŸ” 4ë‹¨ê³„: ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±

### 4.1 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
```python
from typing import List, Dict, Any
import re

class HybridRetriever:
    """
    ì˜ë¯¸ ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    """
    
    def __init__(self, vector_store: SermonVectorStore, embedding_generator: SermonEmbeddingGenerator):
        self.vector_store = vector_store
        self.embedding_generator = embedding_generator
    
    def retrieve_relevant_chunks(
        self, 
        query: str, 
        sermon_collections: List[str],
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        """
        # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_generator.model.encode([query])[0]
        
        # 2. ê° ì„¤êµ ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
        all_results = []
        
        for collection_name in sermon_collections:
            collection = self.vector_store.client.get_collection(collection_name)
            
            # ì˜ë¯¸ ê²€ìƒ‰
            semantic_results = collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=top_k * 2,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í›„ì²˜ë¦¬
                include=["documents", "metadatas", "distances"]
            )
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ (ë‹¨ìˆœ í¬í•¨ ê´€ê³„)
            keyword_results = self._keyword_search(collection, query, top_k)
            
            # ê²°ê³¼ ë³‘í•© ë° ì¬ë­í‚¹
            merged_results = self._merge_and_rerank(
                semantic_results, keyword_results, query
            )
            
            all_results.extend(merged_results)
        
        # ì „ì²´ ê²°ê³¼ì—ì„œ ìƒìœ„ Kê°œ ì„ íƒ
        final_results = sorted(all_results, key=lambda x: x['score'], reverse=True)[:top_k]
        
        return final_results
    
    def _keyword_search(self, collection: chromadb.Collection, query: str, top_k: int) -> Dict:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        """
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ êµ¬í˜„ í•„ìš”)
        keywords = query.split()
        where_clause = {}
        
        # ì„±ê²½ êµ¬ì ˆì´ í¬í•¨ëœ ê²½ìš°
        bible_pattern = r'([ê°€-í£]+ì„œ?)\s*(\d+)ì¥?\s*(\d+)ì ˆ?'
        if re.search(bible_pattern, query):
            # ì„±ê²½ êµ¬ì ˆ í•„í„°ë§ ë¡œì§
            pass
        
        # ì£¼ì œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
        topic_keywords = {
            "ê¸°ë„": ["ê¸°ë„", "ê°„êµ¬", "ì•„ë©˜"],
            "ì‚¬ë‘": ["ì‚¬ë‘", "ìš©ì„œ", "ìë¹„"],
            "ë¯¿ìŒ": ["ë¯¿ìŒ", "ì‹ ì•™", "í™•ì‹ "]
        }
        
        for topic, kwords in topic_keywords.items():
            if any(kw in query for kw in kwords):
                where_clause["topic"] = topic
                break
        
        try:
            results = collection.query(
                query_texts=[query],
                n_results=top_k,
                where=where_clause if where_clause else None,
                include=["documents", "metadatas", "distances"]
            )
            return results
        except:
            return {"documents": [[]], "metadatas": [[]], "distances": [[]]}
    
    def _merge_and_rerank(self, semantic_results: Dict, keyword_results: Dict, query: str) -> List[Dict]:
        """
        ì˜ë¯¸ ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© ë° ì¬ë­í‚¹
        """
        combined_results = {}
        
        # ì˜ë¯¸ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ (ê°€ì¤‘ì¹˜ 0.7)
        for i, (doc, metadata, distance) in enumerate(zip(
            semantic_results["documents"][0],
            semantic_results["metadatas"][0], 
            semantic_results["distances"][0]
        )):
            chunk_id = metadata.get("chunk_index", i)
            score = (1 - distance) * 0.7  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            
            combined_results[chunk_id] = {
                "document": doc,
                "metadata": metadata,
                "score": score,
                "source": "semantic"
            }
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ (ê°€ì¤‘ì¹˜ 0.3)
        for i, (doc, metadata, distance) in enumerate(zip(
            keyword_results["documents"][0],
            keyword_results["metadatas"][0],
            keyword_results["distances"][0]
        )):
            chunk_id = metadata.get("chunk_index", i)
            keyword_score = (1 - distance) * 0.3
            
            if chunk_id in combined_results:
                # ì ìˆ˜ í•©ì‚°
                combined_results[chunk_id]["score"] += keyword_score
                combined_results[chunk_id]["source"] = "hybrid"
            else:
                combined_results[chunk_id] = {
                    "document": doc,
                    "metadata": metadata,
                    "score": keyword_score,
                    "source": "keyword"
                }
        
        return list(combined_results.values())
```

### 4.2 ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œ
```python
from openai import OpenAI

class SermonAnswerGenerator:
    """
    ì„¤êµ ê¸°ë°˜ ë‹µë³€ ìƒì„±ê¸°
    """
    
    def __init__(self, api_key: str, pastor_style: Dict[str, Any]):
        self.client = OpenAI(api_key=api_key)
        self.pastor_style = pastor_style
    
    def generate_answer(
        self, 
        query: str, 
        relevant_chunks: List[Dict[str, Any]]
    ) -> str:
        """
        ê²€ìƒ‰ëœ ì²­í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        """
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(relevant_chunks)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt(query, context)
        
        # LLM í˜¸ì¶œ
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": self._get_system_prompt()},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ê°’
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        answer_with_sources = self._add_source_information(answer, relevant_chunks)
        
        return answer_with_sources
    
    def _get_system_prompt(self) -> str:
        """
        ëª©ì‚¬ë‹˜ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        return f"""
ë‹¹ì‹ ì€ {self.pastor_style.get('name', 'ëª©ì‚¬ë‹˜')}ì˜ ì„¤êµ ìŠ¤íƒ€ì¼ì„ í•™ìŠµí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ëª©ì‚¬ë‹˜ì˜ íŠ¹ì§•:
- ë§íˆ¬: {self.pastor_style.get('tone', 'ë”°ëœ»í•˜ê³  ì¹œê·¼í•¨')}
- ìì£¼ ì‚¬ìš©í•˜ëŠ” í‘œí˜„: {', '.join(self.pastor_style.get('vocabulary', []))}
- ë‹µë³€ êµ¬ì¡°: {self.pastor_style.get('structure', 'ì§ˆë¬¸ ì¸ì • â†’ ì„±ê²½ì  ê·¼ê±° â†’ ì‹¤ì²œì  ì¡°ì–¸')}

ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:
1. ì„±ë„ë‹˜ì„ í–¥í•œ ë”°ëœ»í•œ ë§ˆìŒìœ¼ë¡œ ë‹µë³€
2. ì œê³µëœ ì„¤êµ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ì‚¬ìš©
3. ì„±ê²½ì  ê·¼ê±°ì™€ ì‹¤ì²œì  ì¡°ì–¸ì„ ê· í˜•ìˆê²Œ ì œê³µ
4. ëª©ì‚¬ë‹˜ì˜ ë§íˆ¬ì™€ ìŠ¤íƒ€ì¼ ìœ ì§€
5. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ê²¸ì†í•˜ê²Œ ì¸ì •
"""
    
    def _build_context(self, relevant_chunks: List[Dict[str, Any]]) -> str:
        """
        ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        """
        context_parts = []
        
        for i, chunk in enumerate(relevant_chunks):
            metadata = chunk["metadata"]
            document = chunk["document"]
            
            context_part = f"""
[ì„¤êµ {i+1}]
ì œëª©: {metadata.get('sermon_title', 'ì œëª© ì—†ìŒ')}
ì¼ì‹œ: {metadata.get('sermon_date', 'ë‚ ì§œ ì—†ìŒ')}
ë‚´ìš©: {document}
ì‹œê°„: {metadata.get('start_time', 0):.0f}ì´ˆ - {metadata.get('end_time', 0):.0f}ì´ˆ
"""
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str) -> str:
        """
        ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        """
        return f"""
ì„±ë„ë‹˜ì˜ ì§ˆë¬¸:
{query}

ê´€ë ¨ ì„¤êµ ë‚´ìš©:
{context}

ìœ„ì˜ ì„¤êµ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ëª©ì‚¬ë‹˜ì˜ ìŠ¤íƒ€ì¼ë¡œ ì„±ë„ë‹˜ì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ í›„ì—ëŠ” ì°¸ê³ í•œ ì„¤êµì˜ ì¶œì²˜ ì •ë³´ë„ í¬í•¨í•´ì£¼ì„¸ìš”.
"""
    
    def _add_source_information(
        self, 
        answer: str, 
        relevant_chunks: List[Dict[str, Any]]
    ) -> str:
        """
        ë‹µë³€ì— ì¶œì²˜ ì •ë³´ ì¶”ê°€
        """
        sources = []
        
        for chunk in relevant_chunks:
            metadata = chunk["metadata"]
            source_info = f"""
ğŸ“¹ **{metadata.get('sermon_title', 'ì œëª© ì—†ìŒ')}** ({metadata.get('sermon_date', 'ë‚ ì§œ ì—†ìŒ')})
   â”” ğŸ”— [{int(metadata.get('start_time', 0)//60)}ë¶„ {int(metadata.get('start_time', 0)%60)}ì´ˆ]({metadata.get('timestamp_url', '#')})
"""
            sources.append(source_info)
        
        source_section = "\n".join(sources)
        
        return f"""
{answer}

---

**ğŸ“š ì°¸ê³ í•œ ì„¤êµ:**
{source_section}
"""
```

## ğŸ“Š 5ë‹¨ê³„: í‰ê°€ ë° ìµœì í™”

### 5.1 RAG ì„±ëŠ¥ í‰ê°€
```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

class RAGEvaluator:
    """
    RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€
    """
    
    def __init__(self):
        self.metrics = [faithfulness, answer_relevancy, context_precision]
    
    def evaluate_system(
        self, 
        test_questions: List[str],
        generated_answers: List[str],
        retrieved_contexts: List[List[str]],
        ground_truth_answers: List[str] = None
    ) -> Dict[str, float]:
        """
        RAGAsë¥¼ ì‚¬ìš©í•œ ì‹œìŠ¤í…œ í‰ê°€
        """
        evaluation_data = {
            "question": test_questions,
            "answer": generated_answers,
            "contexts": retrieved_contexts,
        }
        
        if ground_truth_answers:
            evaluation_data["ground_truths"] = ground_truth_answers
        
        results = evaluate(
            dataset=Dataset.from_dict(evaluation_data),
            metrics=self.metrics
        )
        
        return results
```

### 5.2 ì„±ëŠ¥ ìµœì í™” ì „ëµ
```python
class RAGOptimizer:
    """
    RAG ì„±ëŠ¥ ìµœì í™”
    """
    
    def optimize_chunk_size(self, texts: List[str], test_queries: List[str]):
        """
        ìµœì  ì²­í¬ í¬ê¸° ì°¾ê¸°
        """
        chunk_sizes = [400, 600, 800, 1000, 1200]
        best_score = 0
        best_size = 800
        
        for size in chunk_sizes:
            # ì²­í‚¹ í…ŒìŠ¤íŠ¸
            score = self._evaluate_chunk_size(texts, test_queries, size)
            if score > best_score:
                best_score = score
                best_size = size
        
        return best_size
    
    def optimize_retrieval_parameters(self):
        """
        ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìµœì í™” (top_k, ì„ê³„ê°’ ë“±)
        """
        # Grid search ë˜ëŠ” ë² ì´ì§€ì•ˆ ìµœì í™” êµ¬í˜„
        pass
```

## ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©

```python
class SermonRAGPipeline:
    """
    ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ í†µí•© í´ë˜ìŠ¤
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.vector_store = SermonVectorStore(config["vector_db_path"])
        self.embedding_generator = SermonEmbeddingGenerator(config["embedding_model"])
        self.retriever = HybridRetriever(self.vector_store, self.embedding_generator)
        self.answer_generator = SermonAnswerGenerator(
            config["openai_api_key"], 
            config["pastor_style"]
        )
    
    def process_new_sermon(self, youtube_url: str) -> str:
        """
        ìƒˆë¡œìš´ ì„¤êµ ì˜ìƒ ì²˜ë¦¬
        """
        # 1. ì˜ìƒ ë‹¤ìš´ë¡œë“œ
        metadata = download_sermon_video(youtube_url)
        
        # 2. STT ì²˜ë¦¬
        audio_file = f"data/raw_videos/{metadata.title}.mp3"
        transcript = transcribe_audio(audio_file)
        
        # 3. í…ìŠ¤íŠ¸ ì²­í‚¹
        splitter = SermonAwareTextSplitter()
        chunks = splitter.split_sermon_text(transcript.full_text, transcript.segments)
        
        # 4. ì„ë² ë”© ìƒì„±
        embeddings = self.embedding_generator.generate_embeddings(chunks)
        
        # 5. ë²¡í„° DBì— ì €ì¥
        collection = self.vector_store.create_sermon_collection(metadata.youtube_id)
        self.vector_store.add_chunks_to_collection(collection, chunks, embeddings, metadata)
        
        return f"ì„¤êµ '{metadata.title}' ì²˜ë¦¬ ì™„ë£Œ"
    
    def answer_question(self, question: str) -> str:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        """
        # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        collections = self._get_available_collections()
        relevant_chunks = self.retriever.retrieve_relevant_chunks(
            question, collections, top_k=5
        )
        
        # 2. ë‹µë³€ ìƒì„±
        answer = self.answer_generator.generate_answer(question, relevant_chunks)
        
        return answer
    
    def _get_available_collections(self) -> List[str]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ì„¤êµ ì»¬ë ‰ì…˜ ëª©ë¡ ë°˜í™˜
        """
        collections = self.vector_store.client.list_collections()
        return [col.name for col in collections]
```

---

**ë¬¸ì„œ ë²„ì „**: v1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-12-28  
**ë‹¤ìŒ ê²€í†  ì˜ˆì •**: 2025-01-15 